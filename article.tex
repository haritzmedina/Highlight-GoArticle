
%Document generated using DScaffolding from https://www.mindmeister.com/1196948586
\documentclass{article}
\usepackage[utf8]{inputenc}

\newcommand{\todo}[1] {\iffalse #1 \fi} %Use \todo{} command for bringing ideas back to the mind map

\title{Highlight&Go}
\author{}

\begin{document}

\maketitle
      

\section{Introduction}

%Describe the practice in which the problem addressed appears
A key aspect of Extract data from primary studies is that it is It is an iterative process. Apart from that, Extracted data is in textual form or through the use of a set of classification schemes is a fundamental property of Extract data from primary studies. It is collaborative process is another important aspect of Extract data from primary studies. With respect to this, it has been reported that Independent extraction by multiple reviewers, A lone researcher (PhD) and supervisor for a sample of studies and Test-retest approach. Apart from that, It is inextricably linked to data synthesis in qualitative SLRs is a fundamental property of Extract data from primary studies. Extract data from primary studies encompasses different activities: Define classification scheme, Codification of primary studies, Redefinition of new categories/themes, Validation of data extraction and Reuse of outcomes in next phases or by third parties. As for Define classification scheme, it has been described as Classification scheme will be redefined during the whole process. It is conducted using QDA Tools (such as Atlas.TI). As far as Codification of primary studies is concerned, it is conducted using QDA Tools (such as NVivo and RQDA) and Spreadsheets and/or PDF reader (such as Google Sheets, Microsoft Excel, Zotero, Mendeley and Adobe reader). As regards Redefinition of new categories/themes, it has been described as Multiple approaches to reach to a classification scheme: deductive coding, inductive (thematic analysis, grounded theory...), mixed approach,.... It is conducted using QDA Tools (such as Nvivo). As for Validation of data extraction, it has been described as Different methods to support cross checking and "The commonly used procedures for data extraction and validation are mostly the same as those described for quality assessment". It is conducted using Spreadsheets (such as "One option for doing cross validation in practice is to use Excel  (or  Google  Docs) "), SLR tool (such as SLuRp) and QDA tools (such as CAT). As far as Reuse of outcomes in next phases or by third parties is concerned, it is conducted using Exporting results from QDA Tools (such as Nvivo to CSV). 
    
%Describe the practical problem addressed, its significance and its causes
Research has shown that a major problem within Extract data from primary studies is that Interoperability limitations of current SLR tools \cite{Tell2016} \cite{Al-Zubidy2017} \cite{Tell2016} \cite{Ramezani2017}. This problem is of particular concern as it is now well established that it can lead to reuse is hindered \cite{Al-Zubidy2017} \cite{Ramezani2017}, evolution is hindered \cite{Al-Zubidy2017} \cite{Al-Zubidy2017} \cite{Ekaputra2015} and tool migration is hindered \cite{Al-Zubidy2017} \cite{Al-Zubidy2017} \cite{Al-Zubidy2017} \cite{Subramanian2018}. Causes can be diverse: (1) lack of standards \cite{Yueming Sun2012} \cite{Barat2017} \cite{Gilbert2014}, (2) lack of flexible tools \cite{Tell2016} \cite{Tell2016} and (3) lack of a global repositories \cite{Ekaputra2015}. 
    
%Summarise existing research including knowledge gaps and give an account for similar and/or alternative solutions to the problem
Existing research has tackled these causes. Barat et al. addressed the lack of standards \cite{Barat2017}. However, this approach has the following limitations: Manual instantiation of the meta-model using a modeling tool (e.g.xModeler) and It is an approach, They aim to extend an existing SLR specific software, named SLRTool. Barat et al. addressed the lack of standards \cite{Barat2017}. However, this approach has the following limitation: They have also evaluate the possibility of using model-based techniques in following process steps, but it is not presented in the paper. Yueming Sun et al. addressed the lack of standards \cite{Yueming Sun2012}. However, this approach has the following limitations: Manual instantiation of data extracted and It only considers primary studies metadata. Ekaputra et al. addressed the lack of standards \cite{Ekaputra2015}. However, this approach has the following limitation: The tool is not built yet. Corti et al. addressed the lack of standards \cite{Corti2011}. However, this approach has the following limitations: Currently users are unable to export their annotated collection out of their chosen package to retain for archiving purposes or to display in open source tools or publish on the web, say in HTML. [4] and Unless there is a move towards more robust interchangeability—export features and import from a common format, enriched data could be locked into a software system that may be inaccessible and redundant in the future.. Corti et al. addressed the lack of standards \cite{Corti2011}. Corti et al. addressed the lack of standards \cite{Corti2011}. LU et al. addressed the lack of flexible tools \cite{LU2008}. Lu et al. addressed the lack of flexible tools \cite{Lu2008}. However, this approach has the following limitation: the requirements for the input data-set format \cite{Lu2008}. Ramezani et al. addressed the lack of flexible tools \cite{Ramezani2017}. Corti et al. addressed the lack of flexible tools \cite{Corti2011}. 
    
%Formulate goals and present Kernel theories used as a basis for the artefact design
In this work, we address 2 main causes: lack of standards and lack of flexible tools. To lessen these causes, we resort to Semantic web annotations, open: software and platform independent, "a dedicated tool is favorable in order to speed up the annotator training and the annotation process. and One click annotation (OCA strategy). 
    
%Describe the kind of artefact that is developed or evaluated
This article presents an artefact named Highlight&Go. This artefact is a Highlight&Go is a browser extension to annotate PDF and online papers using color-coding. It provides a configurable sidebar, to facilitate the coding and data extraction of primary studies. Finally, it creates a view in Google Sheets to show the data extraction outcome, disagreement detection and traceability of evidences.. 
    
%Formulate research questions
In summary, along Wieringa's template \cite{Wieringa2014}, this paper's design problem can be enunciated as follows: 
improve Interoperability limitations of current SLR tools
by designing a(n) Highlight&Go is a browser extension to annotate PDF and online papers using color-coding. It provides a configurable sidebar, to facilitate the coding and data extraction of primary studies. Finally, it creates a view in Google Sheets to show the data extraction outcome, disagreement detection and traceability of evidences.
that satisfies Usability, Flexibility, Interoperability and Collaborative
in order to help  and undefined. 
    
%Summarize the contributions and their significance
The importance and originality of this study lie in the extension and refinement of existing design knowledge for adapting it to a problem context for which it was not originally intended. Consequently, this study can be classified as an exaptation along Gregor and Hevner’s DSR knowledge contribution framework \cite{Gregor2013}.
      
%Overview of the research strategies and methods used
This article has followed a Design Science Research approach.

%Describe the structure of the paper
The remainder of the paper is structured as follows: 

%Optional - illustrate the relevance and significance of the problem with an example
    
      
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
    